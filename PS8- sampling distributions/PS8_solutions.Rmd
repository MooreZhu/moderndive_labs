
---
title: "Problem Set 08"
author: "WRITE YOUR NAME HERE"
date: "WRITE DATE HERE"
output:
  html_document:
    theme: lumen
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: false
    df_print: kable
---

```{r, include=FALSE}
# Do not edit this code block/chunk
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2)
```


<br> 

## Collaboration {-}

Please indicate who you collaborated with on this problem set:


## Background {-}

For this exercise, we will mimic the tactile sampling you did in class with virtual sampling. We will use some data from the [general social survey](http://gss.norc.org/), an annual personal-interview survey conducted in the United States. The survey is designed to monitor changes in both social characteristics and attitudes. 

For this exercise, the **population** of interest will be **ALL** 2538 individuals living in a single neighborhood in 2014. As an analogy to the tactile sampling you did in class, the neighborhood is the "bowl" and the 2,538 people are the little balls.

If you get stuck as you are working through this Problem Set, it will be helpful to go back and read Chapter 8 in ModernDive. 

## Setup

First load the necessary packages:

```{r}
library(ggplot2)
library(dplyr)
library(forcats)
library(moderndive)
```

You can load and take a `glimpse()` of the  `gss_cat` data set in the `forcats` package like so: 

```{r include = F}
data(gss_cat)
glimpse(gss_cat)
```

Be sure to take a look at the data in the **viewer**. And type `?gss_cat` into the **console** to see a description of the variables in this data set. 

<br> 

***

## Exploratory data wrangling

This data set includes many of years of data, and many variables. To start, we will restrict our analysis to only 2014, and to only the variable indicating the `marital` status of each respondent. 

```{r}
gss_14 <- gss_cat %>% 
  filter(year == 2014) %>% 
  select(marital)
```

The following shows the different responses for `marital` status:

```{r}
gss_14  %>% 
  distinct(marital) 
```

<br>

***

## The true population proportion $p$ of divorced people

Again, for this exercise, the **population** of interest will be **ALL** 2538 individuals living in this single neighborhood in 2014.  Since we have data on **ALL** 2538 people living in the neighborhood, we can compute the **exact population proportion $p$ of divorced people directly** like so, by using **ALL** the data: 

```{r}
gss_14 %>% 
  summarize(divorced = sum(marital == "Divorced"), 
            n = n()) %>% 
  mutate(p  = divorced / n)
```

<br> 

***

Because this is full **Census** data we can compute the **exact population proportion $p$ of divorced people**. 

<br> 

> Note that no inference to the population is needed. We do not need to use a **sample** to try to infer something about the **true poulation proportion $p$** of divorced people in this neighborhood in 2014. We know that $p$ = 0.16 

<br>

However, for this problem set, we will take repeated samples from this neighborhood population just for the purpose of exploring how how factors like sample size influence **variation in sampling distributions**

***

<br>

## Demo: sampling 50 people in the neighborhood

### a) A single sample

We are first going to use random sampling to **estimate** the true **population** proportion $p$ of the neighborhood that are divorced with only a **sample** of 50 people. 

> This will represent a situation of only having the resources to knock on 50 doors to get responses from people in this neighborhood!

Be sure to look at the results in the viewer.

```{r}
n50_1rep <- gss_14 %>% 
  rep_sample_n(size = 50, reps = 1)
```

<br> 

Next, let's calculate the **sample proportion** $\hat{p}$ of people who identified as `Divorced` in our sample of 50 people. 

```{r}
n50_1rep %>% 
  summarize(divorce_count = sum(marital == "Divorced"), 
            n = n()) %>% 
  mutate(p_hat = divorce_count/ n)
```


This sample proportion $\hat{p}$ is an **estimate**, and our **best guess** of what the **true population** proportion $p$ of `Divorced` people is in this neighborhood, based on a sample of only 50 people. It is reasonably close to the true population proportion $p = 0.16$ we calculated from the full population. 

# Question 1a

Ask two of your classmates what their estimate of $\hat{p}$ was. How do the $\hat{p}$ estimates from different samples compare? 

**Answer:  0.08, 0.12, and 0.21. The p_hat estimates are different from different samples. GRADER: Students will obviously report this differently, just looking for basic comprehension that their is some difference **

# Question 1b

**Why** did everyone get a different estimate?

**Answer: because every person took a different random sample with different people in it** 

<br>

***

Typically we only have the opportunity to collect **one sample** for our study, and so we have to use the amount of variability in our **single sample** as an estimate of the amount of variability we might expect in our results if we had taken a random sample of 50 different people. The **standard error of $\hat{p}$** serves as a measure of **sampling variability** if you only have a **single sample**. The formula for calculating the standard error of $\hat{p}$ is the following:

$$SE_{\hat{p}} = \sqrt{\frac{\hat{p} \times (\hat{p}-1)}{n}}$$

The standard error of $\hat{p}$ can be computed in R like so:

```{r}
n50_1rep %>% 
  summarize(divorce_count = sum(marital == "Divorced"), 
            n = n()) %>% 
  mutate(p_hat = divorce_count/ n, 
         se = sqrt(p_hat * (1 - p_hat) / n))
```

<br>

***

### b) Sampling distribution of $\hat{p}$ for n = 50

If you ran the code chunk that takes a random sample of 50 cases a thousand more times....and wrote down every $\hat{p}$ you got, you would have what is called a "sampling distribution". 

<br> 

> A sampling distribution shows every [or nearly every!] possible result a sampling statistic can have under every [or nearly every!] possible sample **of a given size** from a population.  

<br> 

Instead of running the sampling code chunk over and over, we can run 1000 samples really easily in R.  The following code chunk takes 1000 **different** samples of n = 50 and stores them in the data frame `n50_1000rep`. 

```{r}
n50_1000rep <- gss_14 %>% 
  rep_sample_n(size = 50, reps = 1000)
```

 
Be sure to look at `n50_rep1000` in the data viewer to get a sense of these 1000 samples look like.

<br>

***

# Question 2a

What is the name of the column that identifies which of the 1000 samples each row is from?

**Answer: replicate**

# Question 2b

What is the sample size n for each of the 1000 samples we took? (i.e. how many humans are sampled in each replicate)?

**Answer: 50**

<br> 

*** 

The following code chunk calculates the sample proportion $\hat{p}$ of people who reported they were divorced for each of the **1000 samples** 

```{r}
p_hat_n50_1000rep <- n50_1000rep %>% 
  group_by(replicate) %>% 
  summarize(divorce_count = sum(marital == "Divorced"), 
            n = n()) %>% 
  mutate(p_hat = divorce_count / n)
```

Take a look at the first five rows of the results:

```{r}
p_hat_n50_1000rep %>%
  slice(1:5)
```

### c) Visualize the sampling distribution of $\hat{p}$ for n = 50

We can plot the **sampling distribution** of these 1000 $\hat{p}$ estimates of divorced respondents with a histogram, like so:

```{r}
ggplot(p_hat_n50_1000rep, aes(x = p_hat)) +
  geom_histogram(binwidth = 0.02, color = "black", fill = "aquamarine3") +
  labs(x = "Sample proportion of divorced respondents", 
       title = "Sampling distribution of p-hat based on n = 50") 
```

<br> 

***

# Question 3 

Based on your histogram, what appeared to be a very common value of $\hat{p}$? What was a very uncommon value?

**Answer: 0.18 appeared to be a very common value...there was a really high value around 0.4. GREADER: this will differ from student to student obviously...looking for comprehension that the CENTER of the distribution has more common values, and the tails of the distribution are less common values**

<br> 

***

### d) Mean and standard error of the sampling distribution of $\hat{p}$ for n = 50

Finally we can calculate the mean of the sampling distribution by calculating the mean of all 1000 $\hat{p}$ estimates, and the standard error of the sampling distribution by calculating the standard deviation of all 1000 $\hat{p}$ values like so: 

```{r}
p_hat_n50_1000rep %>% 
  summarize(M = mean(p_hat), 
            SE = sd(p_hat))
```

<br> 

*** 

# Question 4

How do these values compare to the estimates we got above of $\hat{p}$ and SE of $\hat{p}$ for `Divorced` respondents based on your **single** sample of 50 people earlier in this Problem Set?  


**Answer: They are very similar**


# Question 5a

Use the `rep_sample_n` function to collect 1000 virtual samples of size *n* = 15. **BE SURE TO NAME YOUR SAMPLE SOMETHING NEW, TO ENSURE YOU CAN DISTINGUISH IT FROM THE n = 50 SAMPLE ABOVE!**

```{r}
n15_1000rep <- gss_14 %>% 
  rep_sample_n(size = 15, reps = 1000)
```


# Question 5b

Calculate sample proportion $\hat{p}$ of people who reported they were `Divorced` for each replicate of your n = 15 sampling


```{r}
p_hat_n15_1000rep <- n15_1000rep %>% 
  group_by(replicate) %>% 
  summarize(divorce_count = sum(marital == "Divorced"), 
            n = n()) %>% 
  mutate(p_hat = divorce_count / n)
```

# Question 5c

Visualize the sampling distribution of $\hat{p}$ from your n = 15 sampling with a histogram 

```{r}
ggplot(p_hat_n15_1000rep, aes(x = p_hat)) +
  geom_histogram(binwidth = 0.07, color = "black", fill = "sienna3") +
  labs(x = "Sample proportion of divorced respondents",
       title = "Sampling distribution of p-hat  based on n = 15")
```

# Question 5d

calculate the mean of the sampling distribution, and the standard error of your n = 15 sampling distribution 

```{r}
p_hat_n15_1000rep %>% 
  summarize(M = mean(p_hat), 
            SE = sd(p_hat))
```

<br> 

***

# Question 6a

How does the standard error of the n= 15 sampling distribution compare to the standard error of the n = 50 sampling distribution?

**Answer: The standard error is much larger at n = 15. It is nearly has nearly double the value of the SE of the n= 50 sampling distribution.** 

# Question 6b

Explain any observed differences from Question 6a

**Answer: At a sample size of 15 we have less data to represent the population on each "resampling", and less information about the population on each "resampling". As such, the variability in the sampling distribution from one sampling event to the next is larger than at n = 50. Thinking about it in the EXTREME...if we "RESAMPLED" the entire population in every resample, we would get the EXACT same value each time...if we resampled only like 5 people in the pop every time we could get RADICALLY different values from sample to sample.**

<br>

***

# Question 7a

Use the `rep_sample_n` function to collect 1000 virtual samples of size *n* = 600. **Note: BE SURE TO NAME YOUR SAMPLE SOMETHING NEW, TO ENSURE YOU CAN DISTINGUISH IT FROM THE n = 50, and n = 15 SAMPLES ABOVE!**

```{r}
n600_1000rep <- gss_14 %>% 
  rep_sample_n(size = 600, reps = 1000)
```

# Question 7b

Calculate the proportion $\hat{p}$ of people who reported they were `Divorced`for each replicate of your n = 600 sampling

```{r}
p_hat_n600_1000rep <- n600_1000rep %>% 
  group_by(replicate) %>% 
  summarize(n = n(), 
            divorce_count = sum(marital == "Divorced")) %>% 
  mutate(p_hat = divorce_count / n)
```

# Question 7c

calculate the mean of the n = 600 sampling distribution, and the standard error of the n = 600 sampling distribution.

```{r}
p_hat_n600_1000rep %>% 
  summarize(M = mean(p_hat), 
            SE = sd(p_hat))
```


# Question 7d

Was there more **variability** from sample to sample when we took a sample size of 600 or a sample size of 50? **Explain what evidence you have for assessing this**

**Answer: the standard error of the sampling distribution for n = 600 was much smaller than the standard error of the sampling distribution for n = 50....indicating that we had more variability from sample to sample when only collecting 50 samples.**


<br> 

***


# Question 8

Which sampling distribution looked more normally distributed (bell shaped and symmetrical); the one built on n = 15, 50 or 600? **Why?**

**Answer: The sampling distribution for n = 600 looked more normally distributed. This is a key tenet of the central limit theorem: as your sample size increases, your sampling distribution becomes more normally distributed, and narrower (i.e. smaller spread) though the n = 50 one also looked pretty bell-shaped and symmetrical**


<br> 

***


![](http://digital-desert.com/victor-valley/599-bell-mountain-r6760.jpg)
 
 <br>
 
## Estimating $\hat{p}$ and the standard error of $\hat{p}$ from a single sample 

In most instances, we do not have access to the full population as we did in this GSS data; instead we have to take a **sample** to try to say something about the **larger population**. Furthermore, in the real world, we typically only take a **single** sample from the population, due to time or money constraints. 

So how do we calculate a $\hat{p}$ and a standard error of $\hat{p}$ when we only have a single sample, and not 1000 repeated samples? As demonstrated at the very beginning of the Problem Set we:

* estimate $\hat{p}$ from the sample 

* use the formula for the standard error of $\hat{p}$ below, to calculate SE based on a single sample 


$$SE_{\hat{p}} = \sqrt{\frac{\hat{p} \times (\hat{p}-1)}{n}}$$

<br> 

*** 

# Question 9

Imagined we collected only a single small sample of 15 respondents like so: 

```{r}
n15_1rep <- gss_14 %>% 
  rep_sample_n(size = 15, reps = 1)
```


Following the example from the beginning of the Problem Set (roughly line 140), calculate the **sample proportion** $\hat{p}$ of people who identified as `Divorced` based on `n15_1rep`... AS WELL AS the **standard error of $\hat{p}$**

```{r}
n15_1rep %>% 
  summarize(divorce_count = sum(marital == "Divorced"), 
            n = n()) %>% 
  mutate(p_hat = divorce_count/ n, 
         se = sqrt(p_hat * (1 - p_hat) / n))
```

<br> 

> You should have gotten a value quite close to the estimate we made earlier from our sampling distribution for n = 15! Note that when you must calculate a standard error from **only a single sample**, the formula **contains the sample size, n**. The larger the sample size n, the larger the number in the denominator of the SE formula. 

<br> 

***

# Question 10 

**IF** you collected a single sample from 600 respondents, do you think the standard error will be smaller or larger than the one you just calculated for n = 15. **Explain your reasoning** (Note: if you are not sure you can collect a sample and calculate the standard error)

**Answer: I think the standard error will be smaller, because 1) we have a larger sample, so we are representing the population better, so there is less room for variability from sample to sample, 2) we saw a smaller standard error in the sampling distribution of n = 600, 3) with a bigger number for n in the denominator of the SE formula, the SE value goes down...GRADER: answer is acceptable as long as the students answers smaller, and gives one of these reasons** 



***

[//]: (students- please do not delete anything below this line!) 

<style type="text/css">
h1 { /* Header 1 */
  color: Blue;
}}
</style>